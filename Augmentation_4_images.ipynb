{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ra9yzAY6WlO_"
   },
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ljx6OBm1cX56"
   },
   "outputs": [],
   "source": [
    "# pip install tensorflow pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gn_oRthfLk7D"
   },
   "source": [
    "**Mounting the drive containing the images**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24991,
     "status": "ok",
     "timestamp": 1734538100225,
     "user": {
      "displayName": "Chat GPT",
      "userId": "15699450882132055055"
     },
     "user_tz": -60
    },
    "id": "YhdWM4mJdDRP",
    "outputId": "0cef7c64-e704-4702-cbf1-8b1f9529a1e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      " Balanced_Images  'Colab Notebooks'   Final_balanced_cleaned_data.json\t Final_Balanced_Images.zip\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "import json\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# drive.flush_and_unmount()\n",
    "\n",
    "!ls '/content/drive/My Drive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJGUOnUBrvOB"
   },
   "source": [
    "**Extracting the balanced images from the zipped file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39277,
     "status": "ok",
     "timestamp": 1734205611271,
     "user": {
      "displayName": "Chat GPT",
      "userId": "15699450882132055055"
     },
     "user_tz": -60
    },
    "id": "XlRvTI6uruk2",
    "outputId": "482bb7a1-4316-49e9-a268-bb848d3f3750"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images extracted successfully to /content/drive/My Drive/Balanced_Images\n",
      "Extracted files: ['images']\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "zip_file_path = '/content/drive/My Drive/Final_Balanced_Images.zip' \n",
    "\n",
    "extract_dir = '/content/drive/My Drive/Balanced_Images'  \n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(f\"Images extracted successfully to {extract_dir}\")\n",
    "\n",
    "    extracted_files = os.listdir(extract_dir)\n",
    "    print(\"Extracted files:\", extracted_files)\n",
    "except Exception as e:\n",
    "    print(f\"Error extracting zip file: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zI6jECVTvewU"
   },
   "source": [
    "**Counting number of images to make sure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 295,
     "status": "ok",
     "timestamp": 1734206179149,
     "user": {
      "displayName": "Chat GPT",
      "userId": "15699450882132055055"
     },
     "user_tz": -60
    },
    "id": "Dy0esYNSvTTW",
    "outputId": "59a76813-cb65-4557-df32-b53f12410d7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the directory: 3347\n"
     ]
    }
   ],
   "source": [
    "extract_dir = '/content/drive/My Drive/Balanced_Images'\n",
    "\n",
    "valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')\n",
    "image_count = sum([1 for file in os.listdir(extract_dir) if file.lower().endswith(valid_extensions)])\n",
    "\n",
    "print(f\"Number of images in the directory: {image_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zmhc-DjuwSRE"
   },
   "source": [
    "**Counting number of cases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1015,
     "status": "ok",
     "timestamp": 1734206303870,
     "user": {
      "displayName": "Chat GPT",
      "userId": "15699450882132055055"
     },
     "user_tz": -60
    },
    "id": "j0zanNVlv24c",
    "outputId": "bd844397-e309-489b-877e-4c2a905f7139"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 1101\n"
     ]
    }
   ],
   "source": [
    "file_path = '/content/drive/My Drive/Final_Combined_Augmented_Data.json'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "if isinstance(data, list):\n",
    "    record_count = len(data)\n",
    "elif isinstance(data, dict):\n",
    "    record_count = len(data)\n",
    "else:\n",
    "    record_count = 0\n",
    "\n",
    "print(f\"Number of records: {record_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aofvPNkW2Vtd"
   },
   "source": [
    "# **Image Augmentation with duplicating the corresponding records**\n",
    "A new folder named \"Augmented_balanced_images\" is created, the folder contains 4 new versions of the image:\n",
    "- RR (rotated right)\n",
    "- RL (rotated left)\n",
    "- FL (fliped horizontally)\n",
    "- BR (brightness change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1787832,
     "status": "ok",
     "timestamp": 1734540220595,
     "user": {
      "displayName": "Chat GPT",
      "userId": "15699450882132055055"
     },
     "user_tz": -60
    },
    "id": "w4bi5WAQ2UNA",
    "outputId": "6207a4ba-487a-4bb6-ff2c-759b6f28fe68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image augmentation completed. New JSON file created with augmented data.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "image_dir = '/content/drive/My Drive/Balanced_Images' \n",
    "augmented_dir = '/content/drive/My Drive/Augmented_Balanced_Images'\n",
    "\n",
    "os.makedirs(augmented_dir, exist_ok=True)\n",
    "\n",
    "def augment_image(image_path, base_name, save_dir):\n",
    "    \"\"\"\n",
    "    Perform augmentations on the image and save the results.\n",
    "    :param image_path: Path to the original image\n",
    "    :param base_name: Base name of the image (without file extension)\n",
    "    :param save_dir: Directory to save augmented images\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = load_img(image_path)\n",
    "\n",
    "        # Augmentation 1: Rotate right 30 degrees\n",
    "        rr_name = f\"{base_name}_RR.png\"\n",
    "        img.rotate(-30).save(os.path.join(save_dir, rr_name))\n",
    "\n",
    "        # Augmentation 2: Rotate left 30 degrees\n",
    "        rl_name = f\"{base_name}_RL.png\"\n",
    "        img.rotate(30).save(os.path.join(save_dir, rl_name))\n",
    "\n",
    "        # Augmentation 3: Horizontal flip\n",
    "        fl_name = f\"{base_name}_FL.png\"\n",
    "        img.transpose(method=Image.FLIP_LEFT_RIGHT).save(os.path.join(save_dir, fl_name))\n",
    "\n",
    "        # Augmentation 4: Brightness increase\n",
    "        br_name = f\"{base_name}_BR.png\"\n",
    "        brightened = ImageEnhance.Brightness(img).enhance(1.5)  # Increase brightness by 50%\n",
    "        brightened.save(os.path.join(save_dir, br_name))\n",
    "\n",
    "        print(f\"Augmented images for {base_name} saved.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {base_name}: {e}\")\n",
    "\n",
    "for image_name in os.listdir(image_dir):\n",
    "    if image_name.endswith(('.png', '.jpg', '.jpeg')):  \n",
    "        base_name = os.path.splitext(image_name)[0]\n",
    "        image_path = os.path.join(image_dir, image_name)\n",
    "        augment_image(image_path, base_name, augmented_dir)\n",
    "\n",
    "print(\"Image augmentation completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYczB2hl2le7"
   },
   "source": [
    "**A new JSON file named \"Augmented_Data.json\" is created, it contains x4 the instances in the original JSON file, each patient has new four records containing their images with the prefixes (\"_RR\", \"_RL\" , \"_FL\" , \"_BR\")**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 612,
     "status": "ok",
     "timestamp": 1734544921900,
     "user": {
      "displayName": "Chat GPT",
      "userId": "15699450882132055055"
     },
     "user_tz": -60
    },
    "id": "A80ELEiIz2xI",
    "outputId": "420d29e8-2fc9-4644-d404-9f6b1e97eef4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New JSON file created with augmented data including original images.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Paths to directories\n",
    "augmented_dir = '/content/drive/My Drive/Augmented_Balanced_Images'  \n",
    "data_file = '/content/drive/My Drive/Final_balanced_cleaned_data.json'  \n",
    "new_json_file = '/content/drive/My Drive/Augmented_Data.json'  \n",
    "\n",
    "with open(data_file, 'r') as f:\n",
    "    patient_data = json.load(f)\n",
    "\n",
    "def create_augmented_json(patient_data, augmented_dir):\n",
    "    \"\"\"\n",
    "    Create a JSON file where each augmentation creates a new patient instance, including original images.\n",
    "    :param patient_data: Original patient data\n",
    "    :param augmented_dir: Directory containing augmented images\n",
    "    :return: List of augmented patient data\n",
    "    \"\"\"\n",
    "    augmented_data = []\n",
    "\n",
    "    for patient in patient_data:\n",
    "        base_patient = patient.copy()\n",
    "        tac_images = patient.get(\"TAC\", [])\n",
    "        mri_images = patient.get(\"MRI\", [])\n",
    "\n",
    "        original_patient = base_patient.copy()\n",
    "        original_patient[\"TAC\"] = tac_images\n",
    "        original_patient[\"MRI\"] = mri_images\n",
    "        augmented_data.append(original_patient)\n",
    "\n",
    "        for aug_type in [\"RR\", \"RL\", \"FL\", \"BR\"]:\n",
    "            new_patient = base_patient.copy()\n",
    "            new_patient[\"U_id\"] = f\"{new_patient['U_id']}_{aug_type}\"\n",
    "\n",
    "            if tac_images:\n",
    "                new_patient[\"TAC\"] = [f\"{image_name}_{aug_type}\" for image_name in tac_images]\n",
    "\n",
    "            if mri_images:\n",
    "                new_patient[\"MRI\"] = [f\"{image_name}_{aug_type}\" for image_name in mri_images]\n",
    "\n",
    "            augmented_data.append(new_patient)\n",
    "\n",
    "    return augmented_data\n",
    "\n",
    "augmented_data = create_augmented_json(patient_data, augmented_dir)\n",
    "\n",
    "with open(new_json_file, 'w') as f:\n",
    "    json.dump(augmented_data, f, indent=4)\n",
    "\n",
    "print(\"New JSON file created with augmented data including original images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdxkYVy04S7d"
   },
   "source": [
    "**Counting Instances to make sure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 351,
     "status": "ok",
     "timestamp": 1734543188691,
     "user": {
      "displayName": "Chat GPT",
      "userId": "15699450882132055055"
     },
     "user_tz": -60
    },
    "id": "Cu59swyJsd8u",
    "outputId": "c3a41b53-a82c-4dc7-8d99-0c2035b1a274"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances in the original JSON file: 1101\n",
      "Number of instances in the augmented JSON file: 5505\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "original_json_file = '/content/drive/My Drive/Final_balanced_cleaned_data.json'\n",
    "augmented_json_file = '/content/drive/My Drive/Augmented_Data.json'\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def count_instances(json_data):\n",
    "    return len(json_data)\n",
    "\n",
    "original_data = load_json(original_json_file)\n",
    "augmented_data = load_json(augmented_json_file)\n",
    "\n",
    "original_count = count_instances(original_data)\n",
    "augmented_count = count_instances(augmented_data)\n",
    "\n",
    "print(f\"Number of instances in the original JSON file: {original_count}\")\n",
    "print(f\"Number of instances in the augmented JSON file: {augmented_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FAwnJklOnPM"
   },
   "source": [
    "**Adding \"Location\" and \"Location Category\" from the descrptions.json to the case.json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FXAqfgBjO-1X"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "file_path = 'Descriptions.json'  \n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "file_path = 'Final JSON data.json' \n",
    "with open(file_path, 'r',encoding=\"utf-8\") as file1:\n",
    "    data1 = json.load(file1)\n",
    "\n",
    "\n",
    "loc= [{\"u_id\": data[i]['U_id'], \"location\": data[i]['Location'],\"location Category\": data[i]['Location Category']} for i in range(len(data))]\n",
    "\n",
    "for i in range(len(loc)):\n",
    "    for j in range(len(data1)):\n",
    "        if loc[i]['u_id'] in data1[j]['U_id']:\n",
    "            data1[j]['Location']=loc[i]['location']\n",
    "            data1[j]['Location Category']=loc[i]['location Category']\n",
    "\n",
    "\n",
    "output_path = 'Final JSON data.json'\n",
    "with open(output_path, 'w') as file1:\n",
    "    json.dump(data1, file1, indent=4)\n",
    "\n",
    "print(f\"Updated JSON saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRBOF5BoPQVJ"
   },
   "source": [
    "**Removing words containing '\\u' (Unicode escape sequences)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rhG8h1DxPNTV"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def remove_unicode_words(text):\n",
    "    words = text.split() \n",
    "    cleaned_words = [word for word in words if \"\\\\u\" not in word]\n",
    "    return \" \".join(cleaned_words)\n",
    "\n",
    "with open(\"input.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "def clean_json(obj):\n",
    "    if isinstance(obj, dict): \n",
    "        return {key: clean_json(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list): \n",
    "        return [clean_json(element) for element in obj]\n",
    "    elif isinstance(obj, str):\n",
    "        return remove_unicode_words(obj)\n",
    "    return obj \n",
    "\n",
    "cleaned_data = clean_json(data)\n",
    "\n",
    "with open(\"cleaned_output.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(cleaned_data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Words containing '\\\\u' have been removed and the cleaned JSON has been saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWqWzoC3Pal0"
   },
   "source": [
    "**Removing the 'â¢' character**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0Ys8JLZPpWV"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def clean_corrupted_text(text):\n",
    "    corrupted_pattern = re.compile(r'[^\\x00-\\x7F]+') \n",
    "    return corrupted_pattern.sub('', text).replace('\\u2022', '')\n",
    "\n",
    "def clean_json(obj):\n",
    "    if isinstance(obj, dict):  \n",
    "        return {key: clean_json(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):  \n",
    "        return [clean_json(element) for element in obj]\n",
    "    elif isinstance(obj, str):  \n",
    "        return clean_corrupted_text(obj)\n",
    "    return obj  \n",
    "\n",
    "with open(\"Final JSON data.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "cleaned_data = clean_json(data)\n",
    "\n",
    "output_path = \"Final JSON data.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(cleaned_data, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(\"Corrupted characters have been removed and the cleaned JSON has been saved.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
